# GPU Allocation Webhook - å¸¸è§é—®é¢˜è§£ç­” (FAQ)

## ğŸ“š ç›®å½•

1. [Webhook å…¼å®¹æ€§](#webhook-å…¼å®¹æ€§)
2. [å¦‚ä½•ä¸ Webhook äº’åŠ¨](#å¦‚ä½•ä¸-webhook-äº’åŠ¨)
3. [èµ„æºæ£€æŸ¥æœºåˆ¶](#èµ„æºæ£€æŸ¥æœºåˆ¶)
4. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
5. [é«˜çº§è¯é¢˜](#é«˜çº§è¯é¢˜)

---

## Webhook å…¼å®¹æ€§

### Q1: Webhook ä»£ç åªèƒ½ç”¨äº fake-gpu-operator å—ï¼Ÿè¿˜æ˜¯ä¹Ÿèƒ½ç”¨äºçœŸå®çš„ NVIDIA GPUï¼Ÿ

**ç­”æ¡ˆ**: âœ… **Webhook å®Œå…¨å…¼å®¹çœŸå®çš„ NVIDIA GPUï¼**

Webhook ä»£ç æ˜¯**å®Œå…¨é€šç”¨**çš„ï¼Œå¯ä»¥ç›´æ¥ç”¨äºçœŸå®çš„ NVIDIA GPU Operatorï¼Œæ— éœ€ä»»ä½•ä¿®æ”¹ã€‚

**åŸå› **:

1. **ä½¿ç”¨æ ‡å‡† Kubernetes API**
   ```go
   // è¿™æ˜¯æ ‡å‡†çš„ Kubernetes client-go APIï¼Œé€‚ç”¨äºä»»ä½•è®¾å¤‡æ’ä»¶
   nodes, err := w.clientset.CoreV1().Nodes().List(context.Background(), metav1.ListOptions{})
   ```

2. **ä½¿ç”¨å®˜æ–¹ NVIDIA èµ„æºåç§°**
   ```go
   "nvidia.com/mig-2g.20gb"   // å®˜æ–¹ NVIDIA MIG é…ç½®æ–‡ä»¶
   "nvidia.com/mig-1g.10gb"   // å®˜æ–¹ NVIDIA MIG é…ç½®æ–‡ä»¶
   "nvidia.com/gpu"           // å®˜æ–¹ NVIDIA GPU èµ„æº
   ```

3. **æ— è®¾å¤‡æ’ä»¶ä¾èµ–**
   - Webhook åªè¯»å– `node.Status.Allocatable`
   - è¿™æ˜¯ç”±ä»»ä½•è®¾å¤‡æ’ä»¶å¡«å……çš„æ ‡å‡†å­—æ®µ
   - ä¸å…³å¿ƒæ˜¯ fake-gpu-operator è¿˜æ˜¯çœŸå®çš„ NVIDIA GPU Operator

**å¯¹æ¯”è¡¨**:

| ç»„ä»¶ | æµ‹è¯•ç¯å¢ƒ (Fake) | ç”Ÿäº§ç¯å¢ƒ (Real) | Webhook ä»£ç  |
|------|----------------|----------------|-------------|
| Device Plugin | fake-gpu-operator | NVIDIA GPU Operator | âœ… ç›¸åŒ |
| ç¡¬ä»¶ | æ¨¡æ‹Ÿ | çœŸå® GPU | âœ… ç›¸åŒ |
| èµ„æºåç§° | nvidia.com/* | nvidia.com/* | âœ… ç›¸åŒ |
| MIG é…ç½® | run.ai/mig.config | nvidia.com/mig.config | âœ… ä¸å½±å“ webhook |
| Webhook é€»è¾‘ | æ£€æŸ¥ Allocatable | æ£€æŸ¥ Allocatable | âœ… å®Œå…¨ç›¸åŒ |

**ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ­¥éª¤**:

```bash
# 1. å®‰è£…çœŸå®çš„ NVIDIA GPU Operator (æ›¿æ¢ fake-gpu-operator)
helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace

# 2. éƒ¨ç½² Webhook (æ— éœ€ä¿®æ”¹ä»£ç ï¼)
cd webhook
./build-and-deploy.sh

# 3. ä¸€åˆ‡æ­£å¸¸å·¥ä½œï¼
```

**è¯¦ç»†ä¿¡æ¯**: å‚è§ [æµ‹è¯•è¯´æ˜.md - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²éƒ¨åˆ†]

---

## å¦‚ä½•ä¸ Webhook äº’åŠ¨

### Q2: å¦‚ä½•é€šè¿‡ YAML ä¸ Webhook äº’åŠ¨ï¼Ÿ

**ç­”æ¡ˆ**: Webhook æ˜¯**è‡ªåŠ¨è§¦å‘**çš„ï¼Œé€šè¿‡åˆ›å»º Kubernetes èµ„æºæ¥è§¦å‘ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**: ä½ ä¸éœ€è¦"è°ƒç”¨" Webhookï¼Œå®ƒä¼šè‡ªåŠ¨æ‹¦æˆªä½ çš„ Pod åˆ›å»ºè¯·æ±‚ã€‚

**å·¥ä½œæµç¨‹**:

```
ä½ : kubectl apply -f pod.yaml
  â†“
API Server æ¥æ”¶è¯·æ±‚
  â†“
ğŸ”” Webhook è‡ªåŠ¨æ‹¦æˆª (MutatingAdmissionWebhook)
  â†“
Webhook æ£€æŸ¥å¹¶å¯èƒ½ä¿®æ”¹ Pod è§„æ ¼
  â†“
è¿”å›ä¿®æ”¹åçš„ Pod åˆ° API Server
  â†“
Pod è¢«åˆ›å»ºï¼ˆä½¿ç”¨ä¿®æ”¹åçš„è§„æ ¼ï¼‰
```

### äº’åŠ¨æ–¹å¼ 1: åˆ›å»ºå•ä¸ª Pod

**ç¤ºä¾‹ YAML** (`my-gpu-pod.yaml`):

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-ml-training
spec:
  restartPolicy: Never
  containers:
  - name: trainer
    image: pytorch/pytorch:latest
    command: ["python", "train.py"]
    resources:
      requests:
        nvidia.com/mig-2g.20gb: "1"  # è¯·æ±‚ 2g.20gb MIG GPU
      limits:
        nvidia.com/mig-2g.20gb: "1"
```

**åˆ›å»ºå¹¶è§¦å‘ Webhook**:

```bash
kubectl apply -f my-gpu-pod.yaml
```

**æŸ¥çœ‹ Webhook æ˜¯å¦ä¿®æ”¹äº† Pod**:

```bash
# æŸ¥çœ‹é™çº§æ³¨è§£
kubectl get pod my-ml-training \
  -o jsonpath='{.metadata.annotations.gpu-webhook\.k8s\.io/fallback}'
# è¾“å‡º: 2g.20gb->gpu

# æŸ¥çœ‹å®é™…åˆ†é…çš„èµ„æº
kubectl get pod my-ml-training \
  -o jsonpath='{.spec.containers[0].resources}' | jq .
# è¾“å‡º: {"limits":{"nvidia.com/gpu":"1"},"requests":{"nvidia.com/gpu":"1"}}
```

### äº’åŠ¨æ–¹å¼ 2: åˆ›å»º Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-workload
spec:
  replicas: 5
  selector:
    matchLabels:
      app: gpu-app
  template:
    metadata:
      labels:
        app: gpu-app
    spec:
      containers:
      - name: worker
        image: nvidia/cuda:11.8.0-base-ubuntu22.04
        resources:
          requests:
            nvidia.com/mig-2g.20gb: "1"
```

**Webhook ä¼šä¸ºæ¯ä¸ªå‰¯æœ¬ Pod ç‹¬ç«‹åšå†³ç­–ï¼**

### äº’åŠ¨æ–¹å¼ 3: å®æ—¶è§‚å¯Ÿ Webhook å·¥ä½œ

**ç»ˆç«¯ 1** - è§‚å¯Ÿæ—¥å¿—:
```bash
kubectl logs -n gpu-webhook -l app=gpu-webhook -f
```

**ç»ˆç«¯ 2** - åˆ›å»º Pod:
```bash
kubectl apply -f my-pod.yaml
```

**ç»ˆç«¯ 1 ä¼šæ˜¾ç¤º**:
```
I1215 13:00:00.123456 1 main.go:60] Reviewing pod: default/my-pod
I1215 13:00:00.123500 1 main.go:73] Pod default/my-pod requests 2g.20gb MIG
I1215 13:00:00.130000 1 main.go:93] 2g.20gb and 1g.10gb not available, falling back to basic GPU
I1215 13:00:00.130500 1 main.go:170] Applied fallback patch to pod default/my-pod
```

### å¸¸ç”¨éªŒè¯å‘½ä»¤

```bash
# æŸ¥çœ‹æ‰€æœ‰è¢« Webhook ä¿®æ”¹çš„ Pod
kubectl get pods \
  -o custom-columns=\
NAME:.metadata.name,\
FALLBACK:.metadata.annotations.gpu-webhook\\.k8s\\.io/fallback

# æŸ¥çœ‹ Pod çš„å®é™…èµ„æºåˆ†é…
kubectl get pod <name> -o yaml | grep -A 10 "resources:"

# æ‰¹é‡æŸ¥çœ‹ Deployment çš„æ‰€æœ‰ Pod
kubectl get pods -l app=myapp \
  -o custom-columns=\
NAME:.metadata.name,\
STATUS:.status.phase,\
FALLBACK:.metadata.annotations.gpu-webhook\\.k8s\\.io/fallback
```

**è¯¦ç»†æ•™ç¨‹**: å‚è§ [æµ‹è¯•è¯´æ˜.md - å¦‚ä½•ä¸ Webhook äº’åŠ¨éƒ¨åˆ†]

---

## èµ„æºæ£€æŸ¥æœºåˆ¶

### Q3: Webhook å¦‚ä½•æ£€æŸ¥å½“å‰å¯ç”¨çš„èµ„æºï¼Ÿ

**ç­”æ¡ˆ**: Webhook æ£€æŸ¥ **Node.Status.Allocatable**ï¼Œè¿™æ˜¯èŠ‚ç‚¹çš„**æ€»å¯åˆ†é…å®¹é‡**ï¼Œä¸æ˜¯**å®é™…å‰©ä½™å¯ç”¨é‡**ã€‚

### æ ¸å¿ƒä»£ç 

```go
func (w *GPUAllocationWebhook) checkMIGAvailability(resourceName string) (bool, error) {
    // æ­¥éª¤ 1: è·å–æ‰€æœ‰èŠ‚ç‚¹
    nodes, err := w.clientset.CoreV1().Nodes().List(context.Background(), metav1.ListOptions{})

    // æ­¥éª¤ 2: éå†èŠ‚ç‚¹
    for _, node := range nodes.Items {
        // æ­¥éª¤ 3: æ£€æŸ¥ Allocatable (æ€»å¯åˆ†é…é‡)
        if allocatable, exists := node.Status.Allocatable[corev1.ResourceName(resourceName)]; exists {
            // æ­¥éª¤ 4: å¦‚æœ >= 1ï¼Œè¿”å› true
            if allocatable.Cmp(resource.MustParse("1")) >= 0 {
                return true, nil  // âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯æ€»é‡ï¼Œä¸æ˜¯å¯ç”¨é‡
            }
        }
    }

    return false, nil  // æ²¡æœ‰èŠ‚ç‚¹æœ‰è¿™ç§èµ„æº
}
```

### Kubernetes èŠ‚ç‚¹èµ„æºå­—æ®µ

æ¯ä¸ªèŠ‚ç‚¹æœ‰ä¸‰ä¸ªå…³é”®èµ„æºå­—æ®µï¼š

```yaml
Node:
  Status:
    Capacity:    # èŠ‚ç‚¹çš„ç‰©ç†èµ„æºæ€»é‡
      nvidia.com/gpu: "4"

    Allocatable: # å‡å»ç³»ç»Ÿä¿ç•™åå¯åˆ†é…çš„èµ„æº (Webhook æ£€æŸ¥è¿™ä¸ª)
      nvidia.com/gpu: "4"

    # Allocated: (æ²¡æœ‰è¿™ä¸ªå­—æ®µï¼éœ€è¦è®¡ç®—)
    # Available:  (æ²¡æœ‰è¿™ä¸ªå­—æ®µï¼éœ€è¦è®¡ç®—)
```

### âš ï¸ é‡è¦ï¼šAllocatable â‰  Available

**Allocatable (å¯åˆ†é…æ€»é‡)** - Webhook çœ‹åˆ°çš„:
- è¿™æ˜¯é™æ€çš„æ€»å®¹é‡
- æ°¸è¿œæ˜¯ 4ï¼ˆå‡è®¾èŠ‚ç‚¹æœ‰ 4 ä¸ª GPUï¼‰
- ä¸ç®¡å·²ç»åˆ†é…äº†å¤šå°‘

**Available (å®é™…å¯ç”¨é‡)** - Webhook çœ‹ä¸åˆ°çš„:
- éœ€è¦è®¡ç®—ï¼š`Available = Allocatable - Allocated`
- ä¼šéšç€ Pod çš„åˆ›å»ºå’Œåˆ é™¤åŠ¨æ€å˜åŒ–
- è¿™æ˜¯ Scheduler ä½¿ç”¨çš„å€¼

### å®é™…åœºæ™¯æ¼”ç¤º

```
åˆå§‹çŠ¶æ€:
â”œâ”€ Worker Node
â”‚   â”œâ”€ Allocatable: 4 GPU  â† Webhook æ£€æŸ¥è¿™ä¸ª
â”‚   â”œâ”€ Allocated:   0 GPU  â† Webhook çœ‹ä¸åˆ°
â”‚   â””â”€ Available:   4 GPU  â† å®é™…å¯ç”¨

åˆ›å»º 3 ä¸ª Pod å:
â”œâ”€ Worker Node
â”‚   â”œâ”€ Allocatable: 4 GPU  â† Webhook è¿˜æ˜¯çœ‹åˆ° 4ï¼
â”‚   â”œâ”€ Allocated:   3 GPU  â† Webhook çœ‹ä¸åˆ°
â”‚   â””â”€ Available:   1 GPU  â† å®é™…åªå‰© 1 ä¸ª

å†åˆ›å»º 2 ä¸ª Pod:
â”œâ”€ Worker Node
â”‚   â”œâ”€ Allocatable: 4 GPU  â† Webhook è¿˜æ˜¯çœ‹åˆ° 4ï¼
â”‚   â”œâ”€ Allocated:   4 GPU  â† æ‰€æœ‰ GPU å·²ç”¨å®Œ
â”‚   â””â”€ Available:   0 GPU  â† æ²¡æœ‰å¯ç”¨äº†

ç»§ç»­åˆ›å»º Pod #6:
â”œâ”€ Webhook æ£€æŸ¥: Allocatable = 4 âœ… (è¿”å› true)
â”œâ”€ Webhook åº”ç”¨é™çº§ï¼Œå…è®¸åˆ›å»º
â””â”€ Pod #6 è¢«åˆ›å»ºä½† Pendingï¼ˆScheduler çŸ¥é“æ²¡èµ„æºï¼‰
```

### ä¸ºä»€ä¹ˆ Webhook ä¸æ£€æŸ¥å®é™…å¯ç”¨é‡ï¼Ÿ

#### åŸå›  1: Kubernetes è®¾è®¡å“²å­¦ - è´£ä»»åˆ†ç¦»

| ç»„ä»¶ | é˜¶æ®µ | èŒè´£ | æ£€æŸ¥çš„èµ„æº |
|------|------|------|-----------|
| **Admission Webhook** | Admission | ä¿®æ”¹/éªŒè¯è¯·æ±‚ | èµ„æº**ç±»å‹**æ˜¯å¦å­˜åœ¨ |
| **Scheduler** | Scheduling | è°ƒåº¦å†³ç­– | èµ„æº**æ•°é‡**æ˜¯å¦è¶³å¤Ÿ |
| **Kubelet** | Binding | å®é™…è¿è¡Œ | ç‰©ç†èµ„æºæ˜¯å¦å¯ç”¨ |

Webhook çš„èŒè´£æ˜¯**èµ„æºç±»å‹è½¬æ¢**ï¼Œä¸æ˜¯**èµ„æºå¯ç”¨æ€§éªŒè¯**ã€‚

#### åŸå›  2: Race Condition (ç«æ€æ¡ä»¶)

å³ä½¿ Webhook è®¡ç®—äº†å¯ç”¨é‡ï¼Œä¹Ÿå¯èƒ½å‡ºç°é—®é¢˜ï¼š

```
æ—¶åˆ» T0: Webhook è®¡ç®— available = 1 GPU
æ—¶åˆ» T1: å¦ä¸€ä¸ªå¹¶å‘è¯·æ±‚åˆ†é…äº†è¿™ 1 GPU
æ—¶åˆ» T2: Webhook å…è®¸å½“å‰ Pod åˆ›å»º
æ—¶åˆ» T3: Scheduler å‘ç°æ²¡èµ„æº â†’ Pod Pending
```

åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œèµ„æºå¯ç”¨æ€§æ£€æŸ¥å’Œå®é™…åˆ†é…ä¹‹é—´æ€»æœ‰æ—¶é—´å·®ã€‚

#### åŸå›  3: æ€§èƒ½è€ƒè™‘

è®¡ç®—å®é™…å¯ç”¨é‡éœ€è¦ï¼š

```go
// ä¼ªä»£ç 
func calculateActualAvailable() {
    for each node {
        allocatable := node.Status.Allocatable

        // éœ€è¦åˆ—å‡ºæ‰€æœ‰ Podï¼
        pods := list_all_pods_on_node(node)

        allocated := 0
        for each pod in pods {
            allocated += pod.resources.requests
        }

        available := allocatable - allocated
    }
}
```

**é—®é¢˜**:
- æ¯ä¸ª Pod åˆ›å»ºéƒ½è¦åˆ—å‡ºæ‰€æœ‰ Pod
- å¤§é›†ç¾¤ï¼ˆæ•°åƒä¸ª Podï¼‰ä¼šä¸¥é‡å½±å“æ€§èƒ½
- API Server ä¼šæˆä¸ºç“¶é¢ˆ

### âœ… å½“å‰ Webhook çš„æ­£ç¡®è¡Œä¸º

Webhook çš„è®¾è®¡ç›®æ ‡ï¼š

```go
// Webhook çš„é€»è¾‘ï¼ˆç®€åŒ–ï¼‰
if cluster_has_resource_type("nvidia.com/mig-2g.20gb") {
    // é›†ç¾¤æœ‰è¿™ç§èµ„æºç±»å‹ï¼Œä¸éœ€è¦é™çº§
    return allow_original_request
}

if cluster_has_resource_type("nvidia.com/mig-1g.10gb") {
    // å¯ä»¥é™çº§åˆ° 1g.10gb
    return fallback_to_1g
}

if cluster_has_resource_type("nvidia.com/gpu") {
    // å¯ä»¥é™çº§åˆ°åŸºç¡€ GPU
    return fallback_to_gpu
}

// é›†ç¾¤æ ¹æœ¬æ²¡æœ‰ GPU
return reject_request
```

Webhook åªå…³å¿ƒï¼š
- âœ… èµ„æº**ç±»å‹**æ˜¯å¦å­˜åœ¨
- âœ… å¯ä»¥é™çº§åˆ°ä»€ä¹ˆç±»å‹
- âŒ ä¸å…³å¿ƒæœ‰å¤šå°‘å¯ç”¨

### å®Œæ•´çš„èµ„æºç®¡ç†æµç¨‹

```
ç”¨æˆ·è¯·æ±‚ â†’ API Server â†’ Webhook â†’ etcd â†’ Scheduler â†’ Kubelet
                           â†“                    â†“
                    ä¿®æ”¹èµ„æºç±»å‹          æ£€æŸ¥å®é™…å¯ç”¨é‡
```

**è¯¦ç»†æµç¨‹**:

1. **Webhook é˜¶æ®µ** (Admission):
   ```
   æ£€æŸ¥: nvidia.com/mig-2g.20gb ç±»å‹æ˜¯å¦å­˜åœ¨ï¼Ÿ
   å†³ç­–: éœ€è¦é™çº§åˆ° nvidia.com/gpu
   è¡Œä¸º: ä¿®æ”¹ Pod è§„æ ¼
   ```

2. **Scheduler é˜¶æ®µ** (Scheduling):
   ```
   æ£€æŸ¥: å“ªä¸ªèŠ‚ç‚¹æœ‰è¶³å¤Ÿçš„ nvidia.com/gpu å¯ç”¨ï¼Ÿ
   è®¡ç®—: Worker æœ‰ 1 GPU å¯ç”¨ï¼ŒWorker2 æœ‰ 2 GPU å¯ç”¨
   å†³ç­–: è°ƒåº¦åˆ° Worker2
   ```

3. **Kubelet é˜¶æ®µ** (Running):
   ```
   æ£€æŸ¥: ç‰©ç† GPU æ˜¯å¦å¯ç”¨
   è¡Œä¸º: åˆ†é… GPU ç»™å®¹å™¨
   ```

### éªŒè¯å‘½ä»¤

**æŸ¥çœ‹èŠ‚ç‚¹çš„ Allocatable (Webhook çœ‹åˆ°çš„)**:

```bash
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
GPU-ALLOCATABLE:.status.allocatable.nvidia\\.com/gpu
```

**è®¡ç®—å®é™…å¯ç”¨é‡ (Scheduler ä½¿ç”¨çš„)**:

```bash
#!/bin/bash
node="gpu-sim-cluster-worker"

# Allocatable
allocatable=$(kubectl get node $node -o jsonpath='{.status.allocatable.nvidia\.com/gpu}')

# Allocated (éœ€è¦è®¡ç®—)
allocated=$(kubectl get pods -A --field-selector spec.nodeName=$node -o json | \
  jq '[.items[].spec.containers[].resources.requests["nvidia.com/gpu"] // "0"] |
      map(tonumber) | add')

# Available
available=$((allocatable - allocated))

echo "Node: $node"
echo "  Allocatable (Webhook sees): $allocatable"
echo "  Allocated (to Pods): $allocated"
echo "  Available (Scheduler uses): $available"
```

### ğŸ“ ç»“è®º

**Webhook çš„å½“å‰å®ç°æ˜¯æ­£ç¡®çš„ï¼**

âœ… **ä¼˜ç‚¹**:
- ç®€å•é«˜æ•ˆ
- ç¬¦åˆ Kubernetes è®¾è®¡å“²å­¦
- é¿å… Race Condition
- æ€§èƒ½å¥½

âœ… **Webhook çš„çœŸæ­£ä»·å€¼**:
- æ™ºèƒ½çš„èµ„æºç±»å‹è½¬æ¢
- è‡ªåŠ¨é™çº§ç­–ç•¥
- é€æ˜çš„æ³¨è§£è¿½è¸ª
- æé«˜èµ„æºåˆ©ç”¨ç‡

âœ… **èµ„æºå¯ç”¨æ€§éªŒè¯äº¤ç»™ Scheduler**:
- Scheduler æ˜¯ä¸“é—¨åšè¿™ä¸ªçš„
- æœ‰å®Œæ•´çš„èµ„æºè¿½è¸ª
- æœ‰ä¼˜åŒ–çš„è°ƒåº¦ç®—æ³•

å¦‚æœ Scheduler å‘ç°èµ„æºä¸è¶³ï¼ŒPod ä¼šè¿›å…¥ **Pending** çŠ¶æ€ï¼Œè¿™æ˜¯**æ­£ç¡®ä¸”é¢„æœŸ**çš„è¡Œä¸ºï¼ç”¨æˆ·å¯ä»¥é€šè¿‡æŸ¥çœ‹ Pod Events äº†è§£åŸå› ï¼š

```bash
kubectl describe pod <name> | grep Events -A 10
# è¾“å‡º: Insufficient nvidia.com/gpu
```

---

## æ•…éšœæ’é™¤

### Q4: Pod ä¸€ç›´å¤„äº Pending çŠ¶æ€ï¼Œä½†æœ‰é™çº§æ³¨è§£ï¼Œä¸ºä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆ**: è¿™æ˜¯**æ­£å¸¸è¡Œä¸º**ã€‚Webhook æˆåŠŸåº”ç”¨äº†é™çº§ï¼Œä½† Scheduler å‘ç°æ‰€æœ‰èŠ‚ç‚¹çš„èµ„æºéƒ½å·²è€—å°½ã€‚

**è¯Šæ–­æ­¥éª¤**:

```bash
# 1. æ£€æŸ¥ Pod çŠ¶æ€
kubectl describe pod <name>

# æŸ¥æ‰¾:
# Events: Insufficient nvidia.com/gpu

# 2. æ£€æŸ¥èŠ‚ç‚¹èµ„æºä½¿ç”¨
kubectl describe nodes | grep -A 10 "Allocated resources:"

# 3. æŸ¥çœ‹å½“å‰è¿è¡Œçš„ GPU Pod æ•°é‡
kubectl get pods -A -o json | \
  jq '[.items[] | select(.spec.containers[].resources.requests["nvidia.com/gpu"])] | length'
```

**è§£å†³æ–¹æ¡ˆ**:
- ç­‰å¾…å…¶ä»– Pod å®Œæˆå¹¶é‡Šæ”¾ GPU
- åˆ é™¤ä¸€äº›ä¸é‡è¦çš„ Pod
- å¢åŠ é›†ç¾¤èŠ‚ç‚¹

### Q5: Webhook æ²¡æœ‰ä¿®æ”¹æˆ‘çš„ Podï¼Œä¸ºä»€ä¹ˆï¼Ÿ

**å¯èƒ½åŸå› **:

1. **Webhook æ²¡æœ‰è¿è¡Œ**
   ```bash
   kubectl get pods -n gpu-webhook
   # åº”è¯¥çœ‹åˆ° Running çŠ¶æ€çš„ Pod
   ```

2. **MutatingWebhookConfiguration é…ç½®é”™è¯¯**
   ```bash
   kubectl get mutatingwebhookconfiguration gpu-allocation-webhook -o yaml
   # æ£€æŸ¥ namespaceSelector, rules ç­‰
   ```

3. **Pod è¯·æ±‚çš„èµ„æºä¸åœ¨ Webhook å¤„ç†èŒƒå›´å†…**
   ```yaml
   # Webhook åªå¤„ç†è¿™äº›èµ„æº:
   - nvidia.com/mig-2g.20gb

   # å¦‚æœä½ è¯·æ±‚å…¶ä»–èµ„æºï¼ŒWebhook ä¸ä¼šå¤„ç†:
   - nvidia.com/mig-3g.40gb  # ä¸å¤„ç†
   - custom-gpu-resource     # ä¸å¤„ç†
   ```

4. **è¯ä¹¦è¿‡æœŸæˆ–æ— æ•ˆ**
   ```bash
   kubectl logs -n gpu-webhook -l app=gpu-webhook
   # æŸ¥æ‰¾ TLS é”™è¯¯
   ```

### Q6: Webhook æ—¥å¿—æ˜¾ç¤º "TLS handshake error"ï¼Œæ€ä¹ˆåŠï¼Ÿ

**åŸå› **: è¯ä¹¦é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:

```bash
# é‡æ–°ç”Ÿæˆè¯ä¹¦
cd webhook
./generate-certs.sh

# é‡å¯ Webhook Pod
kubectl rollout restart deployment/gpu-webhook -n gpu-webhook

# éªŒè¯
kubectl get pods -n gpu-webhook
kubectl logs -n gpu-webhook -l app=gpu-webhook
```

---

## é«˜çº§è¯é¢˜

### Q7: Mixed MIG é…ç½®åœ¨çœŸå® NVIDIA ç¯å¢ƒä¸­èƒ½å¦æ­£å¸¸å·¥ä½œï¼Ÿ

**é—®é¢˜**: è§£é‡Šä¸€ä¸‹ mixed MIGs çš„è®¾ç½®åœ¨çœŸçš„ NVIDIA çš„çŠ¶å†µä¼šæˆåŠŸçš„æä¾›å¯¹çš„ MIG or downgrade MIG?

**ç­”æ¡ˆ**: âœ… **Mixed MIG é…ç½®å®Œå…¨æ”¯æŒï¼ŒWebhook åœ¨çœŸå®ç¯å¢ƒä¸­èƒ½å¤ŸæˆåŠŸæä¾›æ­£ç¡®çš„ MIG æˆ–è‡ªåŠ¨é™çº§ï¼**

### Mixed MIG é…ç½®çš„å¯è¡Œæ€§

**ä½ å½“å‰çš„é…ç½®** (2Ã— 2g.20gb + 1Ã— 3g.30gb per card):

```
è®¡ç®—éªŒè¯ï¼š
- 2g.20gb Ã— 2 = 40GB memory + 4 compute slices
- 3g.30gb Ã— 1 = 30GB memory + 3 compute slices
- æ€»è®¡ = 70GB memory + 7 compute slices âœ…

NVIDIA H200 70GB è§„æ ¼ï¼š7 compute slices, 70GB memory
ç»“è®ºï¼šé…ç½®åˆæ³•ä¸”å¯è¡Œï¼
```

NVIDIA MIG **å®Œå…¨æ”¯æŒ**åœ¨åŒä¸€å¼  GPU å¡ä¸Šæ··åˆä¸åŒå¤§å°çš„ MIG å®ä¾‹ã€‚è¿™æ˜¯ MIG è®¾è®¡çš„æ ¸å¿ƒç‰¹æ€§ã€‚

### çœŸå®ç¯å¢ƒä¸­çš„å·¥ä½œåœºæ™¯

#### âœ… åœºæ™¯ 1: æˆåŠŸæä¾›æ­£ç¡®çš„ MIG

```yaml
# ç”¨æˆ·è¯·æ±‚
resources:
  requests:
    nvidia.com/mig-3g.30gb: 1
```

**Webhook æµç¨‹**:
1. æŸ¥è¯¢é›†ç¾¤èŠ‚ç‚¹ `node.status.allocatable`
2. å‘ç° `nvidia.com/mig-3g.30gb: "4"` (å­˜åœ¨)
3. **å†³ç­–**: èµ„æºç±»å‹å­˜åœ¨ â†’ ä¸ä¿®æ”¹è¯·æ±‚
4. **äº¤ç»™ Scheduler**: æŸ¥æ‰¾æœ‰å¯ç”¨ 3g.30gb çš„èŠ‚ç‚¹
5. **ç»“æœ**: Pod æˆåŠŸè·å¾— 3g.30gb MIG âœ…

#### âœ… åœºæ™¯ 2: æ™ºèƒ½é™çº§åˆ°è¾ƒå° MIG

**æƒ…å†µ**: é›†ç¾¤ä¸­æ²¡æœ‰é…ç½® 3g.30gb MIG

```yaml
# ç”¨æˆ·è¯·æ±‚
resources:
  requests:
    nvidia.com/mig-3g.30gb: 1
```

**Webhook é™çº§é“¾**: 3g.30gb â†’ 2g.20gb â†’ 1g.10gb â†’ gpu

1. æ£€æŸ¥ `nvidia.com/mig-3g.30gb` â†’ ä¸å­˜åœ¨
2. æ£€æŸ¥ `nvidia.com/mig-2g.20gb: "8"` â†’ âœ… å­˜åœ¨
3. **ä¿®æ”¹è¯·æ±‚**ä¸º `nvidia.com/mig-2g.20gb: 1`
4. **æ·»åŠ æ³¨è§£**: `gpu-webhook.k8s.io/fallback: "3g.30gb->2g.20gb"`
5. **ç»“æœ**: Pod è·å¾— 2g.20gb MIG âœ…

#### âš ï¸ åœºæ™¯ 3: Webhook çš„è®¾è®¡é™åˆ¶

**é‡è¦ç†è§£**: Webhook åªæ£€æŸ¥**èµ„æºç±»å‹æ˜¯å¦å­˜åœ¨**ï¼Œä¸æ£€æŸ¥**å®é™…å¯ç”¨æ•°é‡**

```
é›†ç¾¤çŠ¶æ€ï¼š
Medium Node:
  nvidia.com/mig-3g.30gb:
    Allocatable: 4  (æ€»å®¹é‡ - Webhook çœ‹åˆ°è¿™ä¸ª)
    Allocated: 4    (å·²ä½¿ç”¨ - Webhook çœ‹ä¸åˆ°)
    Available: 0    (å¯ç”¨ = 0ï¼- Webhook çœ‹ä¸åˆ°)
```

**Webhook è¡Œä¸º**:
- æŸ¥è¯¢åˆ° `allocatable = 4` (>= 1)
- **åˆ¤æ–­**: é›†ç¾¤æœ‰è¿™ç§ç±»å‹çš„ MIG âœ…
- **ä¸ä¿®æ”¹è¯·æ±‚**ï¼ˆè¿™æ˜¯å…³é”®ï¼ï¼‰
- äº¤ç»™ Scheduler å¤„ç†

**Scheduler è¡Œä¸º**:
- å°è¯•æ‰¾å¯ç”¨çš„ 3g.30gb
- å‘ç°æ‰€æœ‰éƒ½åœ¨ä½¿ç”¨ä¸­
- **Pod è¿›å…¥ Pending çŠ¶æ€** âš ï¸

**è¿™æ˜¯æ­£ç¡®çš„è®¾è®¡ï¼åŸå› **:

### ä¸ºä»€ä¹ˆ Webhook ä¸æ£€æŸ¥å®é™…å¯ç”¨é‡ï¼Ÿ

#### 1ï¸âƒ£ èŒè´£åˆ†ç¦» (Separation of Concerns)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kubernetes æ¶æ„è®¾è®¡åŸåˆ™                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚  Admission Webhook (å‡†å…¥é˜¶æ®µ):           â”‚
â”‚  âœ… èµ„æºç±»å‹è½¬æ¢ (3g â†’ 2g â†’ 1g)          â”‚
â”‚  âœ… éªŒè¯è¯·æ±‚åˆæ³•æ€§                        â”‚
â”‚  âœ… æ·»åŠ é»˜è®¤å€¼å’Œæ³¨è§£                      â”‚
â”‚  âŒ ä¸åº”è¯¥ï¼šè°ƒåº¦å†³ç­–ã€èµ„æºåˆ†é…             â”‚
â”‚                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚  Scheduler (è°ƒåº¦é˜¶æ®µ):                   â”‚
â”‚  âœ… å®æ—¶èµ„æºå¯ç”¨æ€§æ£€æŸ¥                    â”‚
â”‚  âœ… èŠ‚ç‚¹é€‰æ‹©ç®—æ³•                          â”‚
â”‚  âœ… èµ„æºç»‘å®šå’Œåˆ†é…                        â”‚
â”‚  âœ… å¤„ç†èµ„æºä¸è¶³ (Pending)                â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2ï¸âƒ£ Race Condition (ç«æ€æ¡ä»¶)

```
æ—¶é—´çº¿ï¼š
T0: Webhook æ£€æŸ¥ â†’ 3g.30gb available = 1 âœ…
T1: å¦ä¸€ä¸ª Pod å¹¶å‘åˆ›å»º â†’ ä½¿ç”¨äº†é‚£ä¸ª 3g.30gb
T2: å½“å‰ Pod åˆ°è¾¾ Scheduler â†’ available = 0 âŒ
T3: Pod Pending (èµ„æºä¸è¶³)
```

å³ä½¿ Webhook è®¡ç®—äº†å¯ç”¨é‡ï¼Œåœ¨åˆ°è¾¾ Scheduler ä¹‹å‰ï¼Œèµ„æºå¯èƒ½å·²è¢«å…¶ä»– Pod ä½¿ç”¨ã€‚è¿™æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿçš„å›ºæœ‰ç‰¹æ€§ã€‚

#### 3ï¸âƒ£ æ€§èƒ½è€ƒè™‘

è®¡ç®—å®é™…å¯ç”¨é‡éœ€è¦ï¼š
```go
// æ¯ä¸ª Pod åˆ›å»ºæ—¶éƒ½è¦æ‰§è¡Œ
1. åˆ—å‡ºæ‰€æœ‰èŠ‚ç‚¹ (kubectl get nodes)
2. åˆ—å‡ºæ‰€æœ‰ Pods (kubectl get pods --all-namespaces)
3. éå†æ¯ä¸ª Podï¼Œç´¯åŠ èµ„æºä½¿ç”¨
4. è®¡ç®—: available = allocatable - allocated
```

å¯¹äºå¤§å‹é›†ç¾¤ (1000+ nodes, 10000+ pods):
- æ¯æ¬¡ Pod åˆ›å»ºå»¶è¿Ÿ **5-10 ç§’**
- API Server è´Ÿè½½æš´å¢
- **ä¸å¯æ¥å—çš„æ€§èƒ½æŸè€—**

#### 4ï¸âƒ£ Webhook çš„çœŸæ­£ä»·å€¼

**Webhook çš„è®¾è®¡ç›®æ ‡**: è·¨ç¯å¢ƒçš„èµ„æºç±»å‹é€‚é…

```
å¤šæ•°æ®ä¸­å¿ƒç¤ºä¾‹ï¼š

DC-A (50 nodes):
  - H200 GPU: æ”¯æŒ 3g.30gb, 2g.20gb, 1g.10gb

DC-B (100 nodes):
  - A100 GPU: åªæ”¯æŒ 2g.20gb, 1g.10gb

DC-C (200 nodes):
  - V100 GPU: ä¸æ”¯æŒ MIGï¼Œåªæœ‰ nvidia.com/gpu
```

**ç”¨æˆ·æäº¤åŒä¸€ä¸ª YAML**:
```yaml
resources:
  requests:
    nvidia.com/mig-3g.30gb: 1
```

**Webhook çš„æ™ºèƒ½é€‚é…**:
```
DC-A: ä¿æŒ 3g.30gb â†’ âœ… H200 æˆåŠŸè°ƒåº¦
DC-B: é™çº§ â†’ 2g.20gb â†’ âœ… A100 æˆåŠŸè°ƒåº¦
DC-C: é™çº§ â†’ gpu â†’ âœ… V100 æˆåŠŸè°ƒåº¦
```

**æ ¸å¿ƒä¼˜åŠ¿**:
- âœ… ç”¨æˆ·æ— éœ€äº†è§£é›†ç¾¤ GPU é…ç½®
- âœ… åŒä¸€ YAML å¤šç¯å¢ƒè¿è¡Œ
- âœ… è‡ªåŠ¨é€‚é…å¯ç”¨çš„ GPU ç±»å‹
- âœ… æé«˜èµ„æºåˆ©ç”¨ç‡

### çœŸå®ç¯å¢ƒæœ€ä½³å®è·µ

#### æ–¹æ¡ˆ 1: ä½¿ç”¨ Node Selector

```yaml
# ä¸ºä¸åŒ MIG é…ç½®çš„èŠ‚ç‚¹æ‰“æ ‡ç­¾
apiVersion: v1
kind: Node
metadata:
  name: gpu-node-large
  labels:
    gpu.nvidia.com/mig-profile: "3g.30gb"
```

```yaml
# Pod æ˜ç¡®æŒ‡å®šèŠ‚ç‚¹ç±»å‹
apiVersion: v1
kind: Pod
metadata:
  name: large-training
spec:
  nodeSelector:
    gpu.nvidia.com/mig-profile: "3g.30gb"
  containers:
  - name: trainer
    resources:
      requests:
        nvidia.com/mig-3g.30gb: 1
```

#### æ–¹æ¡ˆ 2: ä½¿ç”¨ PriorityClass

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority-gpu
value: 1000000
preemptionPolicy: PreemptLowerPriority
```

é«˜ä¼˜å…ˆçº§ Pod å¯ä»¥é©±é€ä½ä¼˜å…ˆçº§ Pod ä»¥è·å–èµ„æºã€‚

#### æ–¹æ¡ˆ 3: Cluster Autoscaler

é…åˆäº‘ç¯å¢ƒçš„è‡ªåŠ¨æ‰©å®¹ï¼Œå½“èµ„æºä¸è¶³æ—¶è‡ªåŠ¨åˆ›å»ºæ–°èŠ‚ç‚¹ã€‚

### ğŸ¯ æ€»ç»“

**Mixed MIG é…ç½®åœ¨çœŸå® NVIDIA ç¯å¢ƒä¸­çš„è¡¨ç°**:

âœ… **å®Œå…¨æ”¯æŒ**: NVIDIA MIG å…è®¸æ··åˆé…ç½®ï¼ˆ2Ã— 2g.20gb + 1Ã— 3g.30gbï¼‰

âœ… **Webhook èƒ½å¤Ÿ**:
- æ£€æµ‹é›†ç¾¤ä¸­å­˜åœ¨çš„ MIG ç±»å‹
- æ™ºèƒ½é™çº§åˆ°å¯ç”¨çš„ MIG ç±»å‹
- æä¾›è·¨ç¯å¢ƒçš„èµ„æºé€‚é…

âœ… **Webhook ä¸ä¼š**:
- æ£€æŸ¥å®æ—¶å¯ç”¨æ•°é‡ï¼ˆè¿™æ˜¯ Scheduler çš„èŒè´£ï¼‰
- ä¿è¯èµ„æºç«‹å³å¯ç”¨ï¼ˆå¯èƒ½ Pendingï¼‰

âœ… **è¿™æ˜¯æ­£ç¡®çš„è®¾è®¡**:
- ç¬¦åˆ Kubernetes æ¶æ„åŸåˆ™
- é¿å… Race Condition
- ä¿è¯æ€§èƒ½
- èŒè´£æ¸…æ™°åˆ†ç¦»

**é…åˆä½¿ç”¨ Node Selectorã€PriorityClassã€Cluster Autoscaler å¯ä»¥æ„å»ºå®Œæ•´çš„ç”Ÿäº§çº§ GPU è°ƒåº¦ç³»ç»Ÿï¼**

---

### Q9: å¦‚ä½•æ‰©å±• Webhook æ”¯æŒæ›´å¤š GPU ç±»å‹ï¼Ÿ

**ç­”æ¡ˆ**: ä¿®æ”¹é™çº§é€»è¾‘ä»¥æ”¯æŒæ›´å¤šèµ„æºç±»å‹ã€‚

**ç¤ºä¾‹** - æ”¯æŒ 3g.40gb MIG:

```go
// åœ¨ webhook/main.go ä¸­æ·»åŠ 
if qty, exists := container.Resources.Requests["nvidia.com/mig-3g.40gb"]; exists && !qty.IsZero() {
    available, _ := w.checkMIGAvailability("nvidia.com/mig-3g.40gb")

    if !available {
        // å°è¯•é™çº§åˆ° 2g.20gb
        fallback2g, _ := w.checkMIGAvailability("nvidia.com/mig-2g.20gb")

        if fallback2g {
            // é™çº§åˆ° 2g.20gb
            fallbackResource = "nvidia.com/mig-2g.20gb"
            fallbackLabel = "3g.40gb->2g.20gb"
        } else {
            // ç»§ç»­ç°æœ‰çš„é™çº§é“¾
            // ...
        }
    }
}
```

### Q10: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒç›‘æ§ Webhookï¼Ÿ

**å»ºè®®çš„ç›‘æ§æ–¹æ¡ˆ**:

1. **æ·»åŠ  Prometheus æŒ‡æ ‡**:
   ```go
   import "github.com/prometheus/client_golang/prometheus"

   var (
       fallbackCounter = prometheus.NewCounterVec(
           prometheus.CounterOpts{
               Name: "gpu_webhook_fallback_total",
               Help: "Total number of GPU fallback operations",
           },
           []string{"from", "to"},
       )
   )
   ```

2. **ä½¿ç”¨ Grafana ä»ªè¡¨æ¿**:
   - é™çº§æ¬¡æ•°ç»Ÿè®¡
   - Webhook å“åº”æ—¶é—´
   - é”™è¯¯ç‡

3. **è®¾ç½®å‘Šè­¦**:
   ```yaml
   # Prometheus Alert Rule
   - alert: HighGPUFallbackRate
     expr: rate(gpu_webhook_fallback_total[5m]) > 10
     annotations:
       summary: "High GPU fallback rate detected"
   ```

### Q11: Webhook ä¼šå½±å“é›†ç¾¤æ€§èƒ½å—ï¼Ÿ

**ç­”æ¡ˆ**: å½±å“å¾ˆå°ã€‚

**æ€§èƒ½æ•°æ®** (åŸºäºæµ‹è¯•):
- å¹³å‡å“åº”æ—¶é—´: < 100ms
- API Server é¢å¤–å»¶è¿Ÿ: ~15-20ms
- å†…å­˜ä½¿ç”¨: < 50Mi
- CPU ä½¿ç”¨: < 10m

**ä¼˜åŒ–å»ºè®®**:
- éƒ¨ç½²å¤šä¸ª Webhook å‰¯æœ¬
- ä½¿ç”¨ Pod åäº²å’Œæ€§åˆ†æ•£åˆ°ä¸åŒèŠ‚ç‚¹
- è®¾ç½®åˆé€‚çš„èµ„æº requests/limits

```yaml
spec:
  replicas: 3
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: gpu-webhook
```

---

## ğŸ¤ è´¡çŒ®

å¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿ï¼š
1. åœ¨ GitHub æ Issue
2. æäº¤ Pull Request æ·»åŠ æ–°çš„ FAQ
3. åˆ†äº«ä½ çš„ä½¿ç”¨ç»éªŒ

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [README.md](README.md) - é¡¹ç›®æ¦‚è¿°å’Œå¿«é€Ÿå¼€å§‹
- [æµ‹è¯•è¯´æ˜.md](æµ‹è¯•è¯´æ˜.md) - è¯¦ç»†çš„æµ‹è¯•æ–‡æ¡£å’Œæ•™ç¨‹
- [TEST_REPORT.md](TEST_REPORT.md) - è‹±æ–‡æµ‹è¯•æŠ¥å‘Š
- [RESOURCE_EXHAUSTION_TEST.md](RESOURCE_EXHAUSTION_TEST.md) - èµ„æºè€—å°½æµ‹è¯•

---

**æœ€åæ›´æ–°**: 2025å¹´12æœˆ15æ—¥
**ç‰ˆæœ¬**: 1.0
